{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23c1b5c-18d0-43a1-acb9-3aefe8214cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "# Author：Yun Zheng\n",
    "# Email：18848234471@163.com\n",
    "# If there are any shortcomings, please forgive me,because this is my first time sharing.\n",
    "##################################################################################################\n",
    "# Data Augmentation - Tablet\n",
    "# package\n",
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Reshape, Flatten, Input\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from keras import regularizers\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import os\n",
    "from keras.layers import BatchNormalization\n",
    "##################################################################################################\n",
    "# Random seed\n",
    "np.random.seed(30)\n",
    "random.seed(30)\n",
    "tf.random.set_seed(30)\n",
    "# Define directories for saving data and images\n",
    "save_dir = '30_1000_0.006'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "# load your dataset\n",
    "df = pd.read_csv('tablet-3-4.csv', header=None)\n",
    "X1 = df.iloc[1:81, 1:].to_numpy()\n",
    "# normalize\n",
    "scaler = MinMaxScaler()\n",
    "scaled_X1 = scaler.fit_transform(X1)\n",
    "# build the generator model\n",
    "generator = Sequential([\n",
    "    Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.006)),\n",
    "    BatchNormalization(),  \n",
    "    Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.006)),\n",
    "    BatchNormalization(),  \n",
    "    Dense(404, activation='sigmoid', kernel_regularizer=regularizers.l2(0.006)),  \n",
    "    Reshape((404, ))  \n",
    "])\n",
    "# compile the discriminator model\n",
    "generator.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0005, beta_1=0.5), metrics=['accuracy'])\n",
    "# build the discriminator model\n",
    "discriminator = Sequential([\n",
    "    Flatten(input_shape=(404, )),\n",
    "    Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.006)),\n",
    "    Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.006)),\n",
    "    Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.006))\n",
    "])\n",
    "# compile the discriminator model\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0005, beta_1=0.5), metrics=['accuracy'])\n",
    "# combine discriminator and generator\n",
    "discriminator.trainable = False  \n",
    "gan_input = Input(shape=(404,))\n",
    "generated_sample = generator(gan_input)\n",
    "gan_output = discriminator(generated_sample)\n",
    "gan = Model(gan_input, gan_output)\n",
    "# compile the Generative Adversarial Network\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0005, beta_1=0.5))\n",
    "# train the GAN model\n",
    "batch_size = 80\n",
    "epochs = 5001\n",
    "for epoch in range(epochs):\n",
    "    # generate fake samples\n",
    "    noise = np.random.rand(batch_size, 404)\n",
    "    generated_samples = generator.predict(noise)\n",
    "    # randomly select real samples\n",
    "    idx = np.random.randint(0, scaled_X1.shape[0], batch_size)\n",
    "    real_samples = scaled_X1[idx]\n",
    "    if len(real_samples) < batch_size:\n",
    "        repeats = int(np.ceil(batch_size / len(real_samples)))\n",
    "        real_samples = np.repeat(real_samples, repeats, axis=0)[:batch_size]\n",
    "    # train the discriminator\n",
    "    d_loss_real = discriminator.train_on_batch(real_samples, np.ones((batch_size, 1)))\n",
    "    d_loss_fake = discriminator.train_on_batch(generated_samples, np.zeros((batch_size, 1)))\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "    # train the generator\n",
    "    noise = np.random.rand(batch_size, 404)\n",
    "    g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "    # output results after each iteration\n",
    "    print(f\"Epoch: {epoch}, D Loss: {d_loss[0]}, G Loss: {g_loss}\")\n",
    "\n",
    "    # save generated data and images every 100 iterations\n",
    "    if epoch % 100 == 0:\n",
    "        generated_samples1 = generator.predict(np.random.rand(1600, 404))\n",
    "        generated_samples2 = scaler.inverse_transform(generated_samples1.reshape(1600, 404))\n",
    "        S1 = generated_samples2\n",
    "        # save generated data\n",
    "        np.save(os.path.join(save_dir, f'generated_samples_epoch_{epoch}.npy'), S1)\n",
    "        # visualize generated sample data\n",
    "        P_subset = X1[:, :2]\n",
    "        Q_subset = S1[:, :2]\n",
    "        # visualize generated data\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.scatter(P_subset[:, 0], P_subset[:, 1], c='b', label='Original Data (P)')\n",
    "        plt.title('Original Data (P)')\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "        plt.legend()\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.scatter(Q_subset[:, 0], Q_subset[:, 1], c='r', label='Generated Data (Q)')\n",
    "        plt.title('Generated Data (Q)')\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_dir, f'generated_samples_plot_epoch_{epoch}.png'))\n",
    "        plt.close()\n",
    "        # visualize data\n",
    "        plt.figure(dpi=100)\n",
    "        mean_1 = np.mean(X1, axis=0)\n",
    "        mean_2 = np.mean(S1, axis=0)\n",
    "        plt.plot(mean_1, 'r')\n",
    "        plt.plot(mean_2, 'b')\n",
    "        plt.savefig(os.path.join(save_dir, f'mean_comparison_epoch_{epoch}.png'))\n",
    "        plt.close()\n",
    "# The way to get s2 just the same as s1\n",
    "###################################################################################################\n",
    "###################################################################################################\n",
    "###################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
